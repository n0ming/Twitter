from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
import random, time, pyperclip
import csv
import requests
from datetime import datetime
import json
from EdgeGPT.EdgeGPT import Chatbot, ConversationStyle
from pprint import pprint
import urllib.request
import re

header = ['Index','Profile','Date','Origin_Content','Translated_Content','ImageUrl','AttachTweet','AttachUrl']
Index=0
sub_link = False
with open("ttwitter.csv", mode='w', encoding='utf-8-sig', newline='') as file:
    writer = csv.writer(file)
    headers = []
    headers=header
    writer.writerow(header) 

#Bing AI 함수
async def main(href):
    try:
        cookies = json.loads(open("./bing_cookies_test.json", encoding="utf-8").read())
        bot = await Chatbot.create(cookies=cookies)
        prompt = href + "Summarize the contents of the URL in Korean"
        response = await bot.ask(prompt=prompt, conversation_style=ConversationStyle.creative, simplify_response=True)
        buf = response["text"]
        await bot.close()
        return buf
    except Exception as e:
        print(f"An error occurred: {e}")
        return None  
#트위터 접속 url을 본 url로 바꿔주는 함수
def extract_real_url(twitter_url):
    try:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')  # 브라우저 창이 뜨지 않도록 headless 모드로 실행
        browser = webdriver.Chrome(options=options)
        browser.get(twitter_url)
        time.sleep(5) 
        real_url = browser.current_url
        browser.quit()
        return real_url
    except Exception as e:
        print(f"An error occurred: {e}")
        return None  
#한국어 번역 함수
def Toko1(egtext):
    client_id = 'cx7ahlDf9_dx1T_JavyS'
    client_secret = 'Ucny4yIyC6'
    kocText = urllib.parse.quote(egtext)
    data = "source=en&target=ko&text=" + kocText
    url = "https://openapi.naver.com/v1/papago/n2mt"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
        "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8"
    }
    response = requests.post(url, headers=headers, data=data.encode("utf-8"))
    if response.status_code == 200:
        result = response.json()
        translated_text = result['message']['result']['translatedText']
    else:
        print(f"An error occurred: {response.status_code}")
    return translated_text
#줄바꿈 없애는 함수
def no_space(text):
    text1 = re.sub('&nbsp; | &nbsp;|\n\n|\t|\r|', '', text)
    text2 = re.sub('\n\n\n', '', text1)
    text3 = re.sub('\n\n', '', text2)
    text3 = re.sub('\n', '', text2)
    text4 = re.sub(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', ' ', text3)
    text5 = re.sub(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\?)\n', '. ', text4)
    return text5
#statu 다음 숫자 가져오는 함수
def extract_status_id_from_url(url):
    try:
        # URL에서 status/ 다음에 있는 숫자 값을 추출하는 정규표현식
        pattern = r'status/(\d+)'
        match = re.search(pattern, url)
        
        if match:
            status_id = match.group(1)
            return int(status_id)
        else:
            return None
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
#소수점 0으로 reset하는 함수
def replace_decimal_with_zero(num):
    str_num = str(num)
    if '.' in str_num:
        integer_part, decimal_part = str_num.split('.')
        if len(decimal_part) > 0 and int(decimal_part[0]) >= 1:
            return int(integer_part)  # Convert back to integer if the decimal part is 1 or greater
    return num  # Return the original number if the condition is not met

def profile(browser):
    buf_profile=''
    for c_box in browser.find_elements(By.CLASS_NAME, "css-901oao.r-1awozwy.r-18jsvk2.r-6koalj.r-37j5jr.r-a023e6.r-b88u0q.r-rjixqe.r-bcqeeo.r-1udh08x.r-3s2u2q.r-qvutc0"):
            try:
                contents = c_box.find_element(By.CLASS_NAME, "css-901oao.css-16my406.r-poiln3.r-bcqeeo.r-qvutc0")
                buf_profile=contents.text
                if sub_link == False:
                    replace_decimal_with_zero(Index)
                    Index= Index + 1.0
                else: 
                    Index = Index +1.0
                break
            except: pass
    return buf_profile
def date(browser):
    month = {"jan": "1월", "feb": "2월", "mar": "3월", "apr": "4월", "may": "5월", "jun": "6월","jul": "7월", "aug": "8월", "sep": "9월", "oct": "10월", "nov": "11월", "dec": "12월"}
    buf_date=''
    for c_box in browser.find_elements(By.CLASS_NAME, "css-1dbjc4n.r-1d09ksm.r-1471scf.r-18u37iz.r-1wbh5a2"):
            try:
                contents = c_box.find_element(By.TAG_NAME, "time")
                date_time = contents.get_attribute("datetime")
                parsed_date_time = datetime.strptime(date_time, "%Y-%m-%dT%H:%M:%S.000Z")
                formatted_date = parsed_date_time.strftime("%Y년 ") + month[parsed_date_time.strftime("%b").lower()] + parsed_date_time.strftime(" %d일")
                buf_date=formatted_date
            except: pass
    return buf_date
def maintext(browser):
    buf_OC, buf_TC= '', '' #OrignContent(OC), TranslateContent(TC)
    for c_box in browser.find_elements(By.CLASS_NAME, "css-1dbjc4n.r-1s2bzr4"):
            try:
                contents = c_box.find_element(By.CLASS_NAME, "css-901oao.r-18jsvk2.r-37j5jr.r-1inkyih.r-16dba41.r-135wba7.r-bcqeeo.r-bnwqim.r-qvutc0")
                OC = no_space(contents.text)
                buf_OC = OC
                TC= Toko1(OC)
                buf_TC=TC
            except: pass
    return buf_OC, buf_TC
def attach_tweet(browser,browser_url):
    buf_AT ='' #AttachTweet's Url(AT)
    for c_box in browser.find_elements(By.CLASS_NAME, "css-1dbjc4n.r-1ets6dv.r-1867qdf.r-rs99b7.r-1loqt21.r-adacv.r-1ny4l3l.r-1udh08x.r-o7ynqc.r-6416eg"): 
            try:
                contents = c_box.find_element(By.TAG_NAME, "a")
                AT = contents.get_attribute("href")
                AT = extract_real_url(AT)
                if AT.startswith("https://twitter.com"):
                    if all(item not in AT for item in ["photo", "profile", "pbs"]):
                        buf_AT = AT
                    elif "photo" in AT:
                        browser_id = extract_status_id_from_url(browser_url)
                        AT_id = extract_status_id_from_url(AT)
                        if browser_id == AT_id: continue
                        else: buf_AT = AT
            except:
                pass
    return buf_AT
def attach_externalUrl(browser):
    buf_AE=''
    for c_box in browser.find_elements(By.CLASS_NAME, "css-4rbku5.css-18t94o4.css-901oao.css-16my406.r-1cvl2hr.r-1loqt21.r-poiln3.r-bcqeeo.r-qvutc0"):
            try:
                AE = c_box.get_attribute("href")
                AE = extract_real_url(AE)
                if AE.startswith("https://twitter.com"):
                    continue
                buf_AE=AE
                #BingAI 쓰는 코드(CAPTURE때문에 제외시킴)
            except:
                pass
    for c_box in browser.find_elements(By.CLASS_NAME, "css-1dbjc4n.r-1ets6dv.r-1867qdf.r-1phboty.r-rs99b7.r-1ny4l3l.r-1udh08x.r-o7ynqc.r-6416eg"): 
            try:
                content = c_box.find_element(By.TAG_NAME, "a")
                AE = content.get_attribute("href")
                if AE.startswith("https://t.co/"):
                    AE = extract_real_url(AE)
                elif AE.startswith("https://twitter.com"):
                    continue
                buf_AE=AE
            except:
                pass
    return buf_AE
def image(browser):
    buf_IU=''
    for c_box in browser.find_elements(By.CLASS_NAME, "css-1dbjc4n.r-9aw3ui"):
            try:
                for content in c_box.find_elements(By.TAG_NAME, "a"):
                    IU = content.get_attribute("href")
                    buf_IU += IU + "\t"
                    # 여기서부터 이미지 다운로드 등의 작업 수행
            except:
                pass
                for content in browser.find_elements(By.TAG_NAME, "img"):
                    IU = content.get_attribute("src") 
                    if not IU.startswith("https://pbs.twimg.com"):
                        continue
                    elif IU.startswith("https://pbs.twimg.com/profile_images"):
                        continue
                    buf_IU+=IU + "\t"
    return buf_IU

def save_csv(browser_url):
    global sub_link
    global Index
    row_data = []
    Profile = Date = Origin_Content = Translated_Content = ImageUrl = AttachTweet = AttachUrl = ''
    Index = 0
    sub_link =False

    while True:
        browser = webdriver.Chrome()
        browser.get(browser_url)
        browser.implicitly_wait(5)
        row_data=[]
        time.sleep(random.randint(3, 5))
        if sub_link == True: Index = Index + 0.1 - 1.0
    
        Profile = profile(browser)                              #Twitter Profile 함수
        Date = date(browser)                                    #Twitter Date 함수
        Origin_Content, Translated_Content = maintext(browser)  #Twitter Content 함수
        AttachTweet = attach_tweet(browser,browser_url)         #Twitter 또다른 Twitter URL 함수
        AttachUrl = attach_externalUrl(browser)                 #Twitter 본문에 잇는 외부 링크 & #Twitter 본문 밖에 있는 외부 링크(ex. 뉴스) 함수
        ImageUrl = image(browser)

        with open("ttwitter.csv", mode='a', encoding='utf-8-sig', newline='') as file:
            writer = csv.writer(file)
            row_data.append(Index)
            row_data.append(Profile)
            row_data.append(Date)
            row_data.append(Origin_Content)
            row_data.append(Translated_Content)
            row_data.append(ImageUrl)
            row_data.append(AttachTweet)
            row_data.append(AttachUrl)
            writer.writerows([row_data])
        browser.close()

        if AttachTweet=='':
            sub_link = False
            break
        else:
            browser_url = AttachTweet
            Profile = Date = Origin_Content = Translated_Content = ImageUrl = AttachTweet = AttachUrl = ''
            sub_link = True
            row_data = []

# 팔로잉 계정 목록을 가져오기 위해 translateY()를 사용하여 페이지를 아래로 스크롤
def scroll_to_bottom():
    SCROLL_BY_HEIGHT = 500
    last_height = 0
    while True:
        browser.execute_script(f"window.scrollBy(0, {SCROLL_BY_HEIGHT});")
        time.sleep(2)
        new_height = browser.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
def small_scroll_to_bottom():
    SCROLL_BY_HEIGHT = 50
    last_height = 0
    while True:
        browser.execute_script(f"window.scrollBy(0, {SCROLL_BY_HEIGHT});")
        time.sleep(0.5)
        new_height = browser.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
def scroll_to_top():
    browser.execute_script("window.scrollTo(0, 0)")
    time.sleep(0.5)
def extract_real_url(twitter_url):
    response = requests.head(twitter_url, allow_redirects=True)
    real_url = response.url
    return real_url
def login(browser):
    # 브라우저를 주소창을 통해 로그인 페이지 접근
    browser.get("https://twitter.com/snleeintern/following")
    browser.implicitly_wait(5)

    # 웹 페이지에서 id, pw 입력창 찾고 kisia 값 입력 후 랜덤시간 대기
    id_ = browser.find_element(By.XPATH, '//input[@type="text"]')
    pyperclip.copy("snleeintern")
    id_.send_keys(Keys.CONTROL, 'v')
    time.sleep(random.randint(1, 2))

    btn_click = browser.find_element(By.CLASS_NAME, "css-18t94o4.css-1dbjc4n.r-sdzlij.r-1phboty.r-rs99b7.r-ywje51.r-usiww2.r-2yi16.r-1qi8awa.r-1ny4l3l.r-ymttw5.r-o7ynqc.r-6416eg.r-lrvibr.r-13qz1uu")
    btn_click.click()

    pw = browser.find_element(By.XPATH, '//input[@type="password"]')
    pyperclip.copy("20111004")
    pw.send_keys(Keys.CONTROL, 'v')
    time.sleep(random.randint(1, 2))
    # 로그인 버튼을 xpath로 찾고 클릭
    login_button = browser.find_element(By.XPATH, '//*[@data-testid="LoginForm_Login_Button"]')
    login_button.click()
def all_following_tweet_url(browser):
    tweet_url =[]
    following_list = browser.find_elements(By.XPATH, '//div[@data-testid="UserCell"]')
    for idx, _ in enumerate(following_list):
        following_list = browser.find_elements(By.XPATH, '//div[@data-testid="UserCell"]')

        following_account = following_list[idx]
        ActionChains(browser).move_to_element(following_account).click().perform()
        browser.implicitly_wait(5)

        for c_box in browser.find_elements(By.CSS_SELECTOR, "article"):
            try:
                contents = c_box.find_element(By.CLASS_NAME, "css-4rbku5.css-18t94o4.css-901oao.r-14j79pv.r-1loqt21.r-xoduu5.r-1q142lx.r-1w6e6rj.r-37j5jr.r-a023e6.r-16dba41.r-9aw3ui.r-rjixqe.r-bcqeeo.r-3s2u2q.r-qvutc0")
                tweet_url.append(contents.get_attribute("href"))
            except:
                pass
        browser.execute_script("window.history.go(-1)")
        time.sleep(random.uniform(2, 4))
    browser.close()
    return tweet_url

options = webdriver.ChromeOptions()
browser = webdriver.Chrome(options=options)
browser.get("https://twitter.com/snleeintern/following")
browser.implicitly_wait(5)
login(browser)
scroll_to_bottom()
all_tweet=all_following_tweet_url(browser)
for url in all_tweet:
    print(url)
    if "http" in url:
        save_csv(url)
    else:
        pass
