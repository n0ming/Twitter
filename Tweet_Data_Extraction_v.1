from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
import random, time, pyperclip
from selenium import webdriver
from selenium.webdriver.common.proxy import Proxy, ProxyType
from selenium.common.exceptions import NoSuchElementException
import re
import csv
from datetime import datetime
with open('tweets_data.csv', 'w', newline='', encoding='utf-8') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(['Price','cotent','url'])

def following_LastData(browser):
    tweets = browser.find_elements(By.CSS_SELECTOR, "article")
    count = 0 
    data_list = [] 

    element = browser.find_element(By.CLASS_NAME,"css-901oao.css-16my406.r-1awozwy.r-18jsvk2.r-6koalj.r-poiln3.r-b88u0q.r-bcqeeo.r-1udh08x.r-3s2u2q.r-qvutc0")
    profile_name = element.text
    for tweet in tweets:
        if count >= 3: 
            break
        time.sleep(4)
        content = tweet.find_element(By.CSS_SELECTOR, "div[lang]").text
        timestamp = tweet.find_element(By.CSS_SELECTOR, "time").get_attribute("datetime")

        first_occurrence = re.search(r'[.\n\t]', content)

        if first_occurrence:
            first_occurrence_index = first_occurrence.start()
            truncated_content = content[:first_occurrence_index] if content[first_occurrence_index] != "." else content[:first_occurrence_index + 1]
        else:
            truncated_content = content

        data_list.append([profile_name, count + 1, timestamp, truncated_content])
        count += 1

    with open('tweets_data.csv', 'a', newline='', encoding='utf-8') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerows(data_list)

# 팔로잉 계정 목록을 가져오기 위해 translateY()를 사용하여 페이지를 아래로 스크롤
def scroll_to_bottom():
    SCROLL_PAUSE_TIME = 1
    last_height = browser.execute_script("return document.body.scrollHeight")
    while True:
        browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(SCROLL_PAUSE_TIME)
        new_height = browser.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

def scroll_to_tweet1(browser):
    SCROLL_BY_HEIGHT = 1000
    browser.execute_script(f"window.scrollBy(0, {SCROLL_BY_HEIGHT});")
    time.sleep(2)

        

def check_word_in_sentence(sentence, target_words):
    price=None
    if re.search("price:", sentence, re.IGNORECASE):
        pattern = r'(?<=\$)(\d{1,3}(?:,\d{3})*|\d+)(?=\D|$)'
        matches = re.findall(pattern, sentence, re.IGNORECASE)
        if matches:
            price = matches[-1]
            price = int(price.replace(',', ''))
    if price is None:
        return False, None
    else:
        return True, price 

# Tor 브라우저 실행
#tor_proxy = "socks5://127.0.0.1:9150"  # Tor 브라우저의 프록시 주소
options = webdriver.ChromeOptions()
#options.add_argument('--proxy-server=%s' % tor_proxy)
browser = webdriver.Chrome(options=options)


# 브라우저를 주소창을 통해 로그인 페이지 접근
browser.get("https://twitter.com/snleeintern/following")
browser.implicitly_wait(5)

time.sleep(random.randint(3, 10))

# 웹 페이지에서 id, pw 입력창 찾고 kisia 값 입력 후 랜덤시간 대기
id_ = browser.find_element(By.XPATH, '//input[@type="text"]')
pyperclip.copy("snleeintern")
id_.send_keys(Keys.CONTROL, 'v')
time.sleep(random.randint(1, 2))

btn_click = browser.find_element(By.CLASS_NAME, "css-18t94o4.css-1dbjc4n.r-sdzlij.r-1phboty.r-rs99b7.r-ywje51.r-usiww2.r-2yi16.r-1qi8awa.r-1ny4l3l.r-ymttw5.r-o7ynqc.r-6416eg.r-lrvibr.r-13qz1uu")
btn_click.click()

pw = browser.find_element(By.XPATH, '//input[@type="password"]')
pyperclip.copy("20111004")
pw.send_keys(Keys.CONTROL, 'v')
time.sleep(random.randint(1, 2))

# 로그인 버튼을 xpath로 찾고 클릭
login_button = browser.find_element(By.XPATH, '//*[@data-testid="LoginForm_Login_Button"]')
login_button.click()
try:
    element = browser.find_element(By.CLASS_NAME, "css-1dbjc4n.r-18u37iz.r-1pi2tsx.r-1wtj0ep.r-u8s1d.r-13qz1uu")
    print (element.text)
    if element.text == "휴대폰 번호":
        print("ok")
        input_element = browser.find_element(By.XPATH, "//input[@type='tel']")
        pyperclip.copy("01099274599")
        input_element.send_keys(Keys.CONTROL, 'v')
        time.sleep(random.randint(1, 2))

        btn_click = browser.find_element(By.XPATH, "//span[contains(text(), '다음')]")
        btn_click.click()
except NoSuchElementException:
    print("휴대폰 번호를 찾을 수 없습니다.")

scroll_to_bottom()


count_check = True
tweet_urls =[]
desired_date = datetime.strptime("2023-08-10T03:00:00.000Z", "%Y-%m-%dT%H:%M:%S.%fZ")
data_list = [] 
prices =[]
# 팔로잉 계정의 리스트를 가져오기
following_list = browser.find_elements(By.XPATH, '//div[@data-testid="UserCell"]')
word = ["Start Price","Price"]
for idx, _ in enumerate(following_list):
    following_list = browser.find_elements(By.XPATH, '//div[@data-testid="UserCell"]')

    following_account = following_list[idx]
    ActionChains(browser).move_to_element(following_account).click().perform()

    time.sleep(4)
    while count_check:
        for c_box in browser.find_elements(By.CSS_SELECTOR, "article"):
            try:
                contents = c_box.find_element(By.CLASS_NAME, "css-4rbku5.css-18t94o4.css-901oao.r-14j79pv.r-1loqt21.r-xoduu5.r-1q142lx.r-1w6e6rj.r-37j5jr.r-a023e6.r-16dba41.r-9aw3ui.r-rjixqe.r-bcqeeo.r-3s2u2q.r-qvutc0")
                tweet_url = contents.get_attribute("href")
                if tweet_url not in tweet_urls:
                    tweet_urls.append(tweet_url)
                    content = c_box.find_element(By.CSS_SELECTOR, "div[lang]").text
                    truncated_content = content
                    print("CHECK!!!!!!!!!\n"+truncated_content)
                    check, price = check_word_in_sentence(truncated_content, word)
                    if check:
                        print(truncated_content)
                        pattern = r'\$(\d[\d,]*)'
                        match = re.search(pattern, truncated_content)
                
                        b = re.search(r'[.\n\t]', truncated_content)

                        if b:
                            first_occurrence_index = b.start()
                            truncated_content = content[:first_occurrence_index] if content[first_occurrence_index] != "." else content[:first_occurrence_index + 1]
                        else:
                            truncated_content = content
                        data_list.append([price,truncated_content,tweet_url])
                        print(data_list)
            except:
                pass
        scroll_to_tweet1(browser)
        with open('tweets_data.csv', 'a', newline='', encoding='utf-8') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerows(data_list)
        data_list=[]
    following_LastData(browser)

    browser.execute_script("window.history.go(-1)")
    time.sleep(random.uniform(2, 4))


browser.close()
